{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnYT/1F/H/ATT/E5+wCjDp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marmutapp/public/blob/main/Learning_Neural_Networks_From_Scratch_With_Some_Math%E2%80%8A_%E2%80%8APart_A_Python_%26%C2%A0Numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "97WbIWK2mfsm"
      },
      "outputs": [],
      "source": [
        "# Dependencies to be used later\n",
        "import numpy as np\n",
        "import random as rd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Data preparation\n",
        "X1 = rd.sample(range(1, 500), 100)# Creates a list of 100 random numbers in the range of 1, 500\n",
        "print(\"number of clicks on your website:\\n\",X1)\n",
        "X2 = rd.sample(range(1, 500), 100)\n",
        "print(\"\\nnumber of people entering a mall:\\n\",X2)\n",
        "x_input = np.array(list(map(lambda m,n:[m,n],X1,X2)))\n",
        "print(\"\\nnumber of clicks on your website, people entering a mall:\\n\", x_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xNEoNdPm7VP",
        "outputId": "64a4bbbb-6217-460d-e35f-46a7987b3d28"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of clicks on your website:\n",
            " [346, 329, 356, 3, 391, 78, 47, 179, 191, 379, 100, 190, 492, 316, 493, 76, 345, 237, 437, 213, 489, 287, 454, 52, 141, 408, 169, 77, 224, 423, 243, 361, 152, 133, 479, 235, 80, 246, 499, 424, 85, 426, 376, 451, 211, 417, 338, 387, 114, 122, 309, 258, 83, 340, 272, 233, 33, 314, 158, 119, 229, 321, 210, 431, 2, 399, 369, 465, 466, 304, 332, 227, 308, 406, 430, 295, 90, 96, 231, 218, 105, 315, 171, 298, 173, 128, 409, 352, 487, 28, 400, 66, 67, 87, 393, 195, 54, 342, 160, 357]\n",
            "\n",
            "number of people entering a mall:\n",
            " [157, 334, 22, 441, 41, 478, 494, 103, 483, 244, 432, 491, 378, 238, 38, 306, 425, 52, 127, 264, 147, 372, 328, 217, 174, 272, 443, 250, 498, 475, 354, 196, 424, 496, 495, 228, 465, 2, 207, 400, 270, 68, 256, 58, 105, 218, 198, 88, 56, 457, 184, 213, 317, 79, 452, 254, 412, 66, 43, 466, 136, 385, 462, 363, 160, 419, 135, 208, 170, 120, 454, 240, 107, 77, 467, 230, 8, 25, 44, 485, 14, 51, 342, 231, 39, 102, 242, 381, 415, 156, 341, 410, 330, 235, 387, 257, 370, 290, 114, 430]\n",
            "\n",
            "number of clicks on your website, people entering a mall:\n",
            " [[346 157]\n",
            " [329 334]\n",
            " [356  22]\n",
            " [  3 441]\n",
            " [391  41]\n",
            " [ 78 478]\n",
            " [ 47 494]\n",
            " [179 103]\n",
            " [191 483]\n",
            " [379 244]\n",
            " [100 432]\n",
            " [190 491]\n",
            " [492 378]\n",
            " [316 238]\n",
            " [493  38]\n",
            " [ 76 306]\n",
            " [345 425]\n",
            " [237  52]\n",
            " [437 127]\n",
            " [213 264]\n",
            " [489 147]\n",
            " [287 372]\n",
            " [454 328]\n",
            " [ 52 217]\n",
            " [141 174]\n",
            " [408 272]\n",
            " [169 443]\n",
            " [ 77 250]\n",
            " [224 498]\n",
            " [423 475]\n",
            " [243 354]\n",
            " [361 196]\n",
            " [152 424]\n",
            " [133 496]\n",
            " [479 495]\n",
            " [235 228]\n",
            " [ 80 465]\n",
            " [246   2]\n",
            " [499 207]\n",
            " [424 400]\n",
            " [ 85 270]\n",
            " [426  68]\n",
            " [376 256]\n",
            " [451  58]\n",
            " [211 105]\n",
            " [417 218]\n",
            " [338 198]\n",
            " [387  88]\n",
            " [114  56]\n",
            " [122 457]\n",
            " [309 184]\n",
            " [258 213]\n",
            " [ 83 317]\n",
            " [340  79]\n",
            " [272 452]\n",
            " [233 254]\n",
            " [ 33 412]\n",
            " [314  66]\n",
            " [158  43]\n",
            " [119 466]\n",
            " [229 136]\n",
            " [321 385]\n",
            " [210 462]\n",
            " [431 363]\n",
            " [  2 160]\n",
            " [399 419]\n",
            " [369 135]\n",
            " [465 208]\n",
            " [466 170]\n",
            " [304 120]\n",
            " [332 454]\n",
            " [227 240]\n",
            " [308 107]\n",
            " [406  77]\n",
            " [430 467]\n",
            " [295 230]\n",
            " [ 90   8]\n",
            " [ 96  25]\n",
            " [231  44]\n",
            " [218 485]\n",
            " [105  14]\n",
            " [315  51]\n",
            " [171 342]\n",
            " [298 231]\n",
            " [173  39]\n",
            " [128 102]\n",
            " [409 242]\n",
            " [352 381]\n",
            " [487 415]\n",
            " [ 28 156]\n",
            " [400 341]\n",
            " [ 66 410]\n",
            " [ 67 330]\n",
            " [ 87 235]\n",
            " [393 387]\n",
            " [195 257]\n",
            " [ 54 370]\n",
            " [342 290]\n",
            " [160 114]\n",
            " [357 430]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Data preparation\n",
        "y_output = np.array(list(map(lambda m:sum(m),x_input))) # Try experimenting with this function and model parameters. For egs. np.array(list(map(lambda m:sum(m)**2,x_input)))\n",
        "y_output = y_output.reshape(np.shape(y_output)[0],1)\n",
        "print(\"\\nThis is the output variable which reflects the number of purchases on each of the 10 days:\\n\",y_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEgkD96enn25",
        "outputId": "771690f6-3ac2-4c9e-d286-5b5f30dd5c28"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This is the output variable which reflects the number of purchases on each of the 10 days:\n",
            " [[503]\n",
            " [663]\n",
            " [378]\n",
            " [444]\n",
            " [432]\n",
            " [556]\n",
            " [541]\n",
            " [282]\n",
            " [674]\n",
            " [623]\n",
            " [532]\n",
            " [681]\n",
            " [870]\n",
            " [554]\n",
            " [531]\n",
            " [382]\n",
            " [770]\n",
            " [289]\n",
            " [564]\n",
            " [477]\n",
            " [636]\n",
            " [659]\n",
            " [782]\n",
            " [269]\n",
            " [315]\n",
            " [680]\n",
            " [612]\n",
            " [327]\n",
            " [722]\n",
            " [898]\n",
            " [597]\n",
            " [557]\n",
            " [576]\n",
            " [629]\n",
            " [974]\n",
            " [463]\n",
            " [545]\n",
            " [248]\n",
            " [706]\n",
            " [824]\n",
            " [355]\n",
            " [494]\n",
            " [632]\n",
            " [509]\n",
            " [316]\n",
            " [635]\n",
            " [536]\n",
            " [475]\n",
            " [170]\n",
            " [579]\n",
            " [493]\n",
            " [471]\n",
            " [400]\n",
            " [419]\n",
            " [724]\n",
            " [487]\n",
            " [445]\n",
            " [380]\n",
            " [201]\n",
            " [585]\n",
            " [365]\n",
            " [706]\n",
            " [672]\n",
            " [794]\n",
            " [162]\n",
            " [818]\n",
            " [504]\n",
            " [673]\n",
            " [636]\n",
            " [424]\n",
            " [786]\n",
            " [467]\n",
            " [415]\n",
            " [483]\n",
            " [897]\n",
            " [525]\n",
            " [ 98]\n",
            " [121]\n",
            " [275]\n",
            " [703]\n",
            " [119]\n",
            " [366]\n",
            " [513]\n",
            " [529]\n",
            " [212]\n",
            " [230]\n",
            " [651]\n",
            " [733]\n",
            " [902]\n",
            " [184]\n",
            " [741]\n",
            " [476]\n",
            " [397]\n",
            " [322]\n",
            " [780]\n",
            " [452]\n",
            " [424]\n",
            " [632]\n",
            " [274]\n",
            " [787]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Scaling - basically normalizing all data values between 0 & 1\n",
        "def linear_downscaling(dataset, min=None, max=None):\n",
        "  if max == None or min == None:\n",
        "    max = np.max(dataset)\n",
        "    min = np.min(dataset)\n",
        "  scaled_down_value = np.array(list(map(lambda x: (x-min)/(max-min), dataset)))\n",
        "  if scaled_down_value.ndim==1:# If it is a one-dimensional array, this will convert it into two dimensional array i.e. (20,) shape becomes (20,1)\n",
        "    scaled_down_value = scaled_down_value.reshape(np.shape(scaled_down_value)[0],1)\n",
        "  return scaled_down_value, min, max# Return min, max aling with scaled value in order to re-scale later\n",
        "\n",
        "def linear_upscaling(dataset, min, max):\n",
        "  return np.array(list(map(lambda x: x*(max-min)+min, dataset)))\n",
        "\n",
        "x_input_0, x_input_min_0, x_input_max_0  = linear_downscaling(x_input[:,0])\n",
        "x_input_1, x_input_min_1, x_input_max_1  = linear_downscaling(x_input[:,1])\n",
        "x_input = np.array(list(map(lambda m,n:[m[0],n[0]],x_input_0,x_input_1)))\n",
        "y_output, y_output_min, y_output_max  = linear_downscaling(y_output)"
      ],
      "metadata": {
        "id": "zc5b_Q9uYFjd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example activation function\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))#Element-wise operation"
      ],
      "metadata": {
        "id": "nXv-G2WLrH5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model shape\n",
        "layer_size_1=3\n",
        "x_input_shape=np.shape(x_input)\n",
        "\n",
        "# Initializing weights & bias to some random values and zeroes respectively\n",
        "weights_1=np.random.randn(x_input_shape[1],layer_size_1)\n",
        "bias_1=np.zeros((1,layer_size_1))\n",
        "\n",
        "# Multiplying weights with 1st input value and adding bias\n",
        "# We're just doing this for the first input X[0:1]\n",
        "z_layer_1 = np.dot(x_input[0:1], weights_1) + bias_1\n",
        "\n",
        "# Applying the activation function right after\n",
        "a_layer_1 = sigmoid(z_layer_1)"
      ],
      "metadata": {
        "id": "LtSZNOOanoi6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do this again.. for another layer\n",
        "# In this case, the output from previous layer becomes our input\n",
        "# Model shape\n",
        "layer_size_2=3\n",
        "a_layer_1_shape=np.shape(a_layer_1)\n",
        "\n",
        "# Initializing weights & bias to some random values and zeroes respectively\n",
        "weights_2=np.random.randn(a_layer_1_shape[1],layer_size_2)\n",
        "bias_2=np.zeros((1,layer_size_2))\n",
        "\n",
        "# Multiplying weights with 1st input value and adding bias\n",
        "z_layer_2 = np.dot(a_layer_1, weights_2) + bias_2\n",
        "\n",
        "# Applying the activation function right after\n",
        "a_layer_2 = sigmoid(z_layer_2)"
      ],
      "metadata": {
        "id": "_aaM9QRvnyqn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converging this to our output value\n",
        "# Last layer is just going to have the shape of our actual output values i.e. Y as it is what we are trying to predict\n",
        "# Model shape\n",
        "Y_layer_size=1#layer_size_3 basically\n",
        "a_layer_2_shape=np.shape(a_layer_2)\n",
        "\n",
        "# Initializing weights & bias to some random values and zeroes respectively\n",
        "weights_3=np.random.randn(a_layer_2_shape[1],Y_layer_size)\n",
        "bias_3=np.zeros((1,Y_layer_size))\n",
        "\n",
        "# Multiplying weights with 1st input value and adding bias\n",
        "z_layer_3 = np.dot(a_layer_2, weights_3) + bias_3\n",
        "\n",
        "# Applying the activation function right after\n",
        "y_generated = sigmoid(z_layer_3)#same as a_layer_3\n",
        "print(\"Predicted y_generated given weights & biases:\", y_generated,\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13wHHRJrn6SL",
        "outputId": "f631e768-4ff1-4496-ea56-939d6ead8e7d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted y_generated given weights & biases: [[0.60550315]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Error function\n",
        "# Standard Mean Square Error\n",
        "def mse(x,y):\n",
        "  #return np.array(list(np.power(x-y,2)/np.shape(x)[0]))\n",
        "  return np.power(x-y,2)/np.shape(x)[0]"
      ],
      "metadata": {
        "id": "sPD2SOBln2YZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error function 1st order derivative\n",
        "def mse_prime(x,y):\n",
        "  return (x - y)"
      ],
      "metadata": {
        "id": "CoasO0JDoGny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation functions & their derivatives\n",
        "def linear(x):\n",
        "  return x\n",
        "\n",
        "def linear_prime(x):\n",
        "  return 1\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "  return sigmoid(x)*(1-sigmoid(x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_prime(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "  return 1 - np.tanh(x)**2"
      ],
      "metadata": {
        "id": "IR6vyrA5oIpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_3 = mse(y_output[0:1], y_generated)# Generalized version is mse(y_output[batch*batch_size:(batch+1)*batch_size], y_generated)\n",
        "print(\"Error between actual Y output & our predicted Y output:\", error_3,\"\\n\")"
      ],
      "metadata": {
        "id": "9gqbd4VkPt65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9049b968-70e8-42cc-eb20-1987d4053fce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error between actual Y output & our predicted Y output: [[0.0204989]] \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Backpropagation\n",
        "learning_rate = 0.1\n",
        "\n",
        "error_3_prime = mse_prime(y_generated, y_output[0:1])\n",
        "error_2_prime = np.dot(error_3_prime,weights_3.T) * sigmoid_prime(z_layer_2)\n",
        "error_1_prime = np.dot(error_2_prime,weights_2.T) * sigmoid_prime(z_layer_1)\n",
        "\n",
        "weights_error_3 = np.dot(a_layer_2.T, error_3_prime)\n",
        "bias_error_3 = error_3_prime\n",
        "weights_error_2 = np.dot(a_layer_1.T, error_2_prime)\n",
        "bias_error_2 = error_2_prime\n",
        "weights_error_1 = np.dot(np.array(x_input)[0:1].T, error_1_prime)\n",
        "bias_error_1 = error_1_prime\n",
        "\n",
        "weights_3 = weights_3 - weights_error_3*learning_rate\n",
        "bias_3 = bias_3 - bias_error_3*learning_rate\n",
        "weights_2 = weights_2 - weights_error_2*learning_rate\n",
        "bias_2 = bias_2 - bias_error_2*learning_rate\n",
        "weights_1 = weights_1 - weights_error_1*learning_rate\n",
        "bias_1 = bias_1 - bias_error_1*learning_rate\n",
        "\n",
        "# We now have the weights & biases updated\n",
        "# We can use forward pass to get new y_generated"
      ],
      "metadata": {
        "id": "CwLEC6x4oM3o"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FULL MODEL WITH FORWARD PASS + BACKPROPAGATION IN LOOP\n",
        "\n",
        "#Clubbing all parameters before loop\n",
        "\n",
        "#Defining layer size or number of neurons for each layer\n",
        "layer_size_1=3\n",
        "layer_size_2=3\n",
        "Y_layer_size=1#layer_size_3 basically\n",
        "\n",
        "# Model parameters\n",
        "batch_size = 16\n",
        "epochs = 20000\n",
        "learning_rate = 0.05\n",
        "\n",
        "# Initializing weights & bias to some random values and zeroes respectively\n",
        "x_input_shape=np.shape(x_input)\n",
        "weights_1=np.random.randn(x_input_shape[1],layer_size_1)\n",
        "bias_1=np.zeros((batch_size,layer_size_1))\n",
        "\n",
        "a_layer_1_shape=[batch_size,layer_size_1]\n",
        "weights_2=np.random.randn(a_layer_1_shape[1],layer_size_2)\n",
        "bias_2=np.zeros((batch_size,layer_size_2))\n",
        "\n",
        "a_layer_2_shape=[batch_size,layer_size_2]\n",
        "weights_3=np.random.randn(a_layer_2_shape[1],Y_layer_size)\n",
        "bias_3=np.zeros((batch_size,Y_layer_size))\n",
        "\n",
        "# The loop for running the neural network\n",
        "for i in range(0,epochs):\n",
        "  start_time=time.time()\n",
        "  for batch in range(0,int(x_input_shape[0]/batch_size)):\n",
        "    # Forward Pass\n",
        "\n",
        "    # Multiplying weights with 1st input value and adding bias and applying the activation function right after for each layer\n",
        "    z_layer_1 = np.dot(x_input[batch*batch_size:(batch+1)*batch_size], weights_1) + bias_1\n",
        "    a_layer_1 = sigmoid(z_layer_1)\n",
        "    z_layer_2 = np.dot(a_layer_1, weights_2) + bias_2\n",
        "    a_layer_2 = sigmoid(z_layer_2)\n",
        "    z_layer_3 = np.dot(a_layer_2, weights_3) + bias_3\n",
        "    y_generated = linear(z_layer_3)#same as a_layer_3 & linear(z_layer_3) function can be replaced with just z_layer_3\n",
        "\n",
        "    error_3 = mse(y_output[batch*batch_size:(batch+1)*batch_size], y_generated)\n",
        "\n",
        "    # Backpropagation\n",
        "    error_3_prime = mse_prime(y_generated, y_output[batch*batch_size:(batch+1)*batch_size]) * linear_prime(z_layer_3)# linear_prime is just 1 & can be removed entirely\n",
        "    error_2_prime = np.dot(error_3_prime, weights_3.T) * sigmoid_prime(z_layer_2)\n",
        "    error_1_prime = np.dot(error_2_prime, weights_2.T) * sigmoid_prime(z_layer_1)\n",
        "\n",
        "    weights_error_3 = np.dot(a_layer_2.T, error_3_prime)\n",
        "    bias_error_3 = error_3_prime\n",
        "    weights_error_2 = np.dot(a_layer_1.T, error_2_prime)\n",
        "    bias_error_2 = error_2_prime\n",
        "    weights_error_1 = np.dot(x_input[batch*batch_size:(batch+1)*batch_size].T, error_1_prime)\n",
        "    bias_error_1 = error_1_prime\n",
        "\n",
        "    weights_3 = weights_3 - weights_error_3*learning_rate\n",
        "    bias_3 = bias_3 - bias_error_3*learning_rate\n",
        "    weights_2 = weights_2 - weights_error_2*learning_rate\n",
        "    bias_2 = bias_2 - bias_error_2*learning_rate\n",
        "    weights_1 = weights_1 - weights_error_1*learning_rate\n",
        "    bias_1 = bias_1 - bias_error_1*learning_rate\n",
        "\n",
        "  try:\n",
        "    if i%int(epochs*0.1)==0 : print(\"\\nepoch:\",i,\"\\terror: \",sum(error_3)[0],\"\\t average time per epoch: %.5f ms\" %((time.time()-start_time)*1000/batch_size))\n",
        "  except ZeroDivisionError:\n",
        "    print(\"\\nepoch:\",i,\"\\terror_3:\",sum(error_3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfcjJu1ooV3O",
        "outputId": "4db68279-283f-49cf-af93-9f7dec6124bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch: 0 \terror:  0.032570203360272464 \t average time per epoch: 0.42453 ms\n",
            "\n",
            "epoch: 2000 \terror:  1.167400742069551e-05 \t average time per epoch: 0.02950 ms\n",
            "\n",
            "epoch: 4000 \terror:  7.553810686555276e-06 \t average time per epoch: 0.02983 ms\n",
            "\n",
            "epoch: 6000 \terror:  6.196504341356965e-06 \t average time per epoch: 0.03049 ms\n",
            "\n",
            "epoch: 8000 \terror:  5.345148603218313e-06 \t average time per epoch: 0.06723 ms\n",
            "\n",
            "epoch: 10000 \terror:  4.7227197009112164e-06 \t average time per epoch: 0.03077 ms\n",
            "\n",
            "epoch: 12000 \terror:  4.2439496306279944e-06 \t average time per epoch: 0.03085 ms\n",
            "\n",
            "epoch: 14000 \terror:  3.865126794450179e-06 \t average time per epoch: 0.03029 ms\n",
            "\n",
            "epoch: 16000 \terror:  3.558894044565183e-06 \t average time per epoch: 0.02965 ms\n",
            "\n",
            "epoch: 18000 \terror:  3.306755201515024e-06 \t average time per epoch: 0.03025 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your neural network if it can predict well after its \"learning\" process\n",
        "# Let's generate a few test points in the same range\n",
        "new_x_input_1 = np.array(rd.sample(range(1, 500), batch_size*2))\n",
        "new_x_input_2 = np.array(rd.sample(range(1, 500), batch_size*2))\n",
        "new_x_input = np.array(list(map(lambda m,n:[m,n],new_x_input_1,new_x_input_2)))\n",
        "print(\"Here are our input values to predict the output for: \\n\", new_x_input)\n",
        "# Scale the inputs using input scaling factor\n",
        "new_x_input_0, _, _  = linear_downscaling(new_x_input[:,0], x_input_min_0, x_input_max_0)\n",
        "new_x_input_1, _, _  = linear_downscaling(new_x_input[:,1], x_input_min_1, x_input_max_1)\n",
        "new_x_input = np.array(list(map(lambda m,n:[m[0],n[0]],new_x_input_0,new_x_input_1)))\n",
        "print(\"Here are our scaled down input values to predict the output for: \\n\", new_x_input)\n",
        "# These are the same variables & structure that we used in the forward pass to get y_generated for new input\n",
        "# When you train, the shape of your internal or hidden layers are fixed as per your batch_size so your test input should also adhere to the batch_size constraint\n",
        "print(\"Here are the corresponding predictions for output values:\\n\")\n",
        "for batch in range(int(np.shape(new_x_input)[0]/batch_size)):\n",
        "  z_layer_1 = np.dot(new_x_input[batch*batch_size:(batch+1)*batch_size], weights_1)   + bias_1\n",
        "  a_layer_1 = sigmoid(z_layer_1)\n",
        "\n",
        "  z_layer_2 = np.dot(a_layer_1, weights_2) + bias_2\n",
        "  a_layer_2 = sigmoid(z_layer_2)\n",
        "\n",
        "  z_layer_3 = np.dot(a_layer_2, weights_3) + bias_3\n",
        "  y_predicted = linear(z_layer_3)#same as a_layer_3\n",
        "\n",
        "  print(linear_upscaling(y_predicted, y_output_min, y_output_max))# y_generated values are downscaled as our training y_output data was also downscaled; We need to upscale it back to get output value as per original levels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDdWk7ORVguO",
        "outputId": "e19a3205-2a10-47a7-b6dd-9a062b3266c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are our input values to predict the output for: \n",
            " [[394  68]\n",
            " [271 422]\n",
            " [456 115]\n",
            " [ 15  72]\n",
            " [482 431]\n",
            " [314 421]\n",
            " [200  94]\n",
            " [196 429]\n",
            " [357 257]\n",
            " [186  13]\n",
            " [ 74 152]\n",
            " [438 424]\n",
            " [155 349]\n",
            " [134 282]\n",
            " [492 123]\n",
            " [483 133]\n",
            " [307 465]\n",
            " [ 44 397]\n",
            " [471 416]\n",
            " [ 60 425]\n",
            " [236 220]\n",
            " [429 109]\n",
            " [ 99 439]\n",
            " [ 51 226]\n",
            " [266 388]\n",
            " [125 344]\n",
            " [484 430]\n",
            " [389   5]\n",
            " [ 96 482]\n",
            " [215 347]\n",
            " [120  25]\n",
            " [399   7]]\n",
            "Here are our scaled down input values to predict the output for: \n",
            " [[ 0.79233871  0.12678937]\n",
            " [ 0.54435484  0.85071575]\n",
            " [ 0.91733871  0.22290389]\n",
            " [ 0.02822581  0.13496933]\n",
            " [ 0.96975806  0.86912065]\n",
            " [ 0.63104839  0.84867076]\n",
            " [ 0.40120968  0.1799591 ]\n",
            " [ 0.39314516  0.86503067]\n",
            " [ 0.71774194  0.51329243]\n",
            " [ 0.37298387  0.01431493]\n",
            " [ 0.14717742  0.29856851]\n",
            " [ 0.88104839  0.85480573]\n",
            " [ 0.31048387  0.70143149]\n",
            " [ 0.26814516  0.56441718]\n",
            " [ 0.98991935  0.2392638 ]\n",
            " [ 0.97177419  0.2597137 ]\n",
            " [ 0.61693548  0.93865031]\n",
            " [ 0.08669355  0.799591  ]\n",
            " [ 0.94758065  0.83844581]\n",
            " [ 0.11895161  0.85685072]\n",
            " [ 0.47379032  0.43762781]\n",
            " [ 0.86290323  0.21063395]\n",
            " [ 0.19758065  0.88548057]\n",
            " [ 0.10080645  0.44989775]\n",
            " [ 0.53427419  0.78118609]\n",
            " [ 0.25        0.69120654]\n",
            " [ 0.97379032  0.86707566]\n",
            " [ 0.78225806 -0.00204499]\n",
            " [ 0.19153226  0.97341513]\n",
            " [ 0.43145161  0.69734151]\n",
            " [ 0.23991935  0.03885481]\n",
            " [ 0.80241935  0.00204499]]\n",
            "Here are the corresponding predictions for output values:\n",
            "\n",
            "[[460.21516214]\n",
            " [697.48240631]\n",
            " [570.44326727]\n",
            " [ 93.78289404]\n",
            " [904.5528269 ]\n",
            " [731.10289194]\n",
            " [297.46315314]\n",
            " [629.62308668]\n",
            " [617.44018767]\n",
            " [198.56262766]\n",
            " [227.16712065]\n",
            " [852.03012001]\n",
            " [504.64624786]\n",
            " [415.49607336]\n",
            " [614.50256397]\n",
            " [614.29586966]]\n",
            "[[760.254782  ]\n",
            " [437.53474942]\n",
            " [869.23524623]\n",
            " [484.69342258]\n",
            " [451.9413083 ]\n",
            " [537.13117571]\n",
            " [535.7488429 ]\n",
            " [275.64569494]\n",
            " [657.79745934]\n",
            " [469.25752149]\n",
            " [889.78437849]\n",
            " [392.34950317]\n",
            " [578.07326928]\n",
            " [564.49512208]\n",
            " [146.5192841 ]\n",
            " [403.9774307 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actual_y_new_output = linear_upscaling(new_x_input[:,0], x_input_min_0, x_input_max_0) + linear_upscaling(new_x_input[:,1], x_input_min_1, x_input_max_1 )# With alternative dataset option of y function -- (linear_upscaling(new_x_input[:,0], x_input_min_0, x_input_max_0) + linear_upscaling(new_x_input[:,1], x_input_min_1, x_input_max_1 ))**2\n",
        "print(\"\\nHere are the actual values based on what we already know the relationship to be:\\n\", actual_y_new_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLZno2aQtPqz",
        "outputId": "64635025-d1b9-4028-982b-01ecc51c29b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Here are the actual values based on what we already know the relationship to be:\n",
            " [462. 693. 571.  87. 913. 735. 294. 625. 614. 199. 226. 862. 504. 416.\n",
            " 615. 616. 772. 441. 887. 485. 456. 538. 538. 277. 654. 469. 914. 394.\n",
            " 578. 562. 145. 406.]\n"
          ]
        }
      ]
    }
  ]
}