{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvXTCROmf2t8jeCKIpaGv1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marmutapp/public/blob/main/Learning_Neural_Networks_From_Scratch_With_Some_Math%E2%80%8A_%E2%80%8APart_A_Python_%26%C2%A0Numpy_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "97WbIWK2mfsm"
      },
      "outputs": [],
      "source": [
        "# Dependencies to be used later\n",
        "import numpy as np\n",
        "import random as rd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Data preparation\n",
        "rd.seed(0)\n",
        "X1 = rd.sample(range(1, 500), 100)# Creates a list of 100 random numbers in the range of 1, 500\n",
        "print(\"number of clicks on your website:\\n\",X1)\n",
        "X2 = rd.sample(range(1, 500), 100)\n",
        "print(\"\\nnumber of people entering a mall:\\n\",X2)\n",
        "x_input = np.array(list(map(lambda m,n:[m,n],X1,X2)))\n",
        "print(\"\\nnumber of clicks on your website, people entering a mall:\\n\", x_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xNEoNdPm7VP",
        "outputId": "0f3555d6-756d-4bab-e935-5a363914c03f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of clicks on your website:\n",
            " [433, 198, 389, 456, 216, 21, 133, 262, 249, 208, 471, 402, 425, 156, 245, 184, 299, 457, 465, 112, 259, 72, 145, 478, 387, 49, 317, 410, 129, 466, 273, 362, 415, 309, 462, 76, 159, 51, 374, 38, 436, 351, 170, 242, 287, 52, 182, 223, 162, 313, 328, 105, 283, 485, 227, 444, 267, 134, 32, 413, 281, 8, 48, 369, 431, 205, 364, 423, 488, 343, 321, 1, 314, 253, 424, 171, 125, 461, 167, 361, 33, 98, 291, 114, 123, 412, 73, 414, 279, 230, 47, 42, 164, 261, 251, 56, 155, 447, 150, 468]\n",
            "\n",
            "number of people entering a mall:\n",
            " [64, 281, 171, 418, 473, 277, 105, 410, 309, 498, 301, 148, 228, 47, 306, 409, 198, 163, 295, 124, 149, 95, 97, 421, 96, 17, 314, 337, 134, 244, 36, 46, 348, 388, 67, 449, 77, 20, 432, 42, 359, 425, 494, 350, 201, 429, 362, 269, 142, 268, 416, 121, 435, 111, 467, 302, 423, 215, 297, 141, 231, 253, 339, 329, 459, 407, 183, 43, 167, 495, 60, 250, 489, 323, 172, 98, 125, 9, 375, 139, 454, 453, 113, 191, 434, 88, 497, 219, 32, 52, 401, 75, 358, 417, 24, 294, 325, 274, 491, 349]\n",
            "\n",
            "number of clicks on your website, people entering a mall:\n",
            " [[433  64]\n",
            " [198 281]\n",
            " [389 171]\n",
            " [456 418]\n",
            " [216 473]\n",
            " [ 21 277]\n",
            " [133 105]\n",
            " [262 410]\n",
            " [249 309]\n",
            " [208 498]\n",
            " [471 301]\n",
            " [402 148]\n",
            " [425 228]\n",
            " [156  47]\n",
            " [245 306]\n",
            " [184 409]\n",
            " [299 198]\n",
            " [457 163]\n",
            " [465 295]\n",
            " [112 124]\n",
            " [259 149]\n",
            " [ 72  95]\n",
            " [145  97]\n",
            " [478 421]\n",
            " [387  96]\n",
            " [ 49  17]\n",
            " [317 314]\n",
            " [410 337]\n",
            " [129 134]\n",
            " [466 244]\n",
            " [273  36]\n",
            " [362  46]\n",
            " [415 348]\n",
            " [309 388]\n",
            " [462  67]\n",
            " [ 76 449]\n",
            " [159  77]\n",
            " [ 51  20]\n",
            " [374 432]\n",
            " [ 38  42]\n",
            " [436 359]\n",
            " [351 425]\n",
            " [170 494]\n",
            " [242 350]\n",
            " [287 201]\n",
            " [ 52 429]\n",
            " [182 362]\n",
            " [223 269]\n",
            " [162 142]\n",
            " [313 268]\n",
            " [328 416]\n",
            " [105 121]\n",
            " [283 435]\n",
            " [485 111]\n",
            " [227 467]\n",
            " [444 302]\n",
            " [267 423]\n",
            " [134 215]\n",
            " [ 32 297]\n",
            " [413 141]\n",
            " [281 231]\n",
            " [  8 253]\n",
            " [ 48 339]\n",
            " [369 329]\n",
            " [431 459]\n",
            " [205 407]\n",
            " [364 183]\n",
            " [423  43]\n",
            " [488 167]\n",
            " [343 495]\n",
            " [321  60]\n",
            " [  1 250]\n",
            " [314 489]\n",
            " [253 323]\n",
            " [424 172]\n",
            " [171  98]\n",
            " [125 125]\n",
            " [461   9]\n",
            " [167 375]\n",
            " [361 139]\n",
            " [ 33 454]\n",
            " [ 98 453]\n",
            " [291 113]\n",
            " [114 191]\n",
            " [123 434]\n",
            " [412  88]\n",
            " [ 73 497]\n",
            " [414 219]\n",
            " [279  32]\n",
            " [230  52]\n",
            " [ 47 401]\n",
            " [ 42  75]\n",
            " [164 358]\n",
            " [261 417]\n",
            " [251  24]\n",
            " [ 56 294]\n",
            " [155 325]\n",
            " [447 274]\n",
            " [150 491]\n",
            " [468 349]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Data preparation\n",
        "#y_output = np.array(list(map(lambda m:sum(m),x_input))) # Try experimenting with this function and model parameters. For egs. np.array(list(map(lambda m:sum(m)**2,x_input)))\n",
        "y_output = np.array(list(map(lambda m:m[0]**2+m[1]+10,x_input)))\n",
        "y_output = y_output.reshape(np.shape(y_output)[0],1)\n",
        "print(\"\\nThis is the output variable which reflects the number of purchases on each of the 10 days:\\n\",y_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEgkD96enn25",
        "outputId": "fe09903c-9e96-499e-ba75-414fab8d1dcf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "This is the output variable which reflects the number of purchases on each of the 10 days:\n",
            " [[187563]\n",
            " [ 39495]\n",
            " [151502]\n",
            " [208364]\n",
            " [ 47139]\n",
            " [   728]\n",
            " [ 17804]\n",
            " [ 69064]\n",
            " [ 62320]\n",
            " [ 43772]\n",
            " [222152]\n",
            " [161762]\n",
            " [180863]\n",
            " [ 24393]\n",
            " [ 60341]\n",
            " [ 34275]\n",
            " [ 89609]\n",
            " [209022]\n",
            " [216530]\n",
            " [ 12678]\n",
            " [ 67240]\n",
            " [  5289]\n",
            " [ 21132]\n",
            " [228915]\n",
            " [149875]\n",
            " [  2428]\n",
            " [100813]\n",
            " [168447]\n",
            " [ 16785]\n",
            " [217410]\n",
            " [ 74575]\n",
            " [131100]\n",
            " [172583]\n",
            " [ 95879]\n",
            " [213521]\n",
            " [  6235]\n",
            " [ 25368]\n",
            " [  2631]\n",
            " [140318]\n",
            " [  1496]\n",
            " [190465]\n",
            " [123636]\n",
            " [ 29404]\n",
            " [ 58924]\n",
            " [ 82580]\n",
            " [  3143]\n",
            " [ 33496]\n",
            " [ 50008]\n",
            " [ 26396]\n",
            " [ 98247]\n",
            " [108010]\n",
            " [ 11156]\n",
            " [ 80534]\n",
            " [235346]\n",
            " [ 52006]\n",
            " [197448]\n",
            " [ 71722]\n",
            " [ 18181]\n",
            " [  1331]\n",
            " [170720]\n",
            " [ 79202]\n",
            " [   327]\n",
            " [  2653]\n",
            " [136500]\n",
            " [186230]\n",
            " [ 42442]\n",
            " [132689]\n",
            " [178982]\n",
            " [238321]\n",
            " [118154]\n",
            " [103111]\n",
            " [   261]\n",
            " [ 99095]\n",
            " [ 64342]\n",
            " [179958]\n",
            " [ 29349]\n",
            " [ 15760]\n",
            " [212540]\n",
            " [ 28274]\n",
            " [130470]\n",
            " [  1553]\n",
            " [ 10067]\n",
            " [ 84804]\n",
            " [ 13197]\n",
            " [ 15573]\n",
            " [169842]\n",
            " [  5836]\n",
            " [171625]\n",
            " [ 77883]\n",
            " [ 52962]\n",
            " [  2620]\n",
            " [  1849]\n",
            " [ 27264]\n",
            " [ 68548]\n",
            " [ 63035]\n",
            " [  3440]\n",
            " [ 24360]\n",
            " [200093]\n",
            " [ 23001]\n",
            " [219383]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Scaling - basically normalizing all data values between 0 & 1\n",
        "def linear_downscaling(dataset, min=None, max=None):\n",
        "  if max == None or min == None:\n",
        "    max = np.max(dataset)\n",
        "    min = np.min(dataset)\n",
        "  scaled_down_value = np.array(list(map(lambda x: (x-min)/(max-min), dataset)))\n",
        "  if scaled_down_value.ndim==1:# If it is a one-dimensional array, this will convert it into two dimensional array i.e. (20,) shape becomes (20,1)\n",
        "    scaled_down_value = scaled_down_value.reshape(np.shape(scaled_down_value)[0],1)\n",
        "  return scaled_down_value, min, max# Return min, max aling with scaled value in order to re-scale later\n",
        "\n",
        "def linear_upscaling(dataset, min, max):\n",
        "  return np.array(list(map(lambda x: x*(max-min)+min, dataset)))\n",
        "\n",
        "x_input_0, x_input_min_0, x_input_max_0  = linear_downscaling(x_input[:,0])\n",
        "x_input_1, x_input_min_1, x_input_max_1  = linear_downscaling(x_input[:,1])\n",
        "x_input = np.array(list(map(lambda m,n:[m[0],n[0]],x_input_0,x_input_1)))\n",
        "y_output, y_output_min, y_output_max  = linear_downscaling(y_output)"
      ],
      "metadata": {
        "id": "zc5b_Q9uYFjd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# abstract base class work\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class activation(ABC):\n",
        "  @abstractmethod\n",
        "  def prime(self):\n",
        "    pass\n",
        "\n",
        "class loss(ABC):\n",
        "  @abstractmethod\n",
        "  def prime(self):\n",
        "    pass\n",
        "\n",
        "class mse(loss):\n",
        "  def __call__(self, y_pred, y_true):\n",
        "    return np.power(y_pred - y_true,2)/2\n",
        "  def prime(self, y_pred, y_true):\n",
        "    return (y_pred - y_true)\n",
        "\n",
        "class sigmoid(activation):\n",
        "  def __call__(self, z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def prime(self, z):\n",
        "    Z=self(z)\n",
        "    return Z*(1-Z)\n",
        "\n",
        "class linear(activation):\n",
        "  def __call__(self, z):\n",
        "    return z\n",
        "  def prime(self, z):\n",
        "    return 1\n",
        "\n",
        "class relu(activation):\n",
        "  def __call__(self, z):\n",
        "      return np.maximum(0, z)\n",
        "  def prime(self, z):\n",
        "      return np.where(z > 0, 1, 0)\n",
        "\n",
        "class tanh(activation):\n",
        "  def __call__(self, z):\n",
        "      return np.tanh(z)\n",
        "  def prime(self, z):\n",
        "      return 1 - self(z)**2\n",
        "\n",
        "class layer:\n",
        "  def __init__(self, size, activation_fn):\n",
        "    self.size = size\n",
        "    self.activation_fn = activation_fn\n",
        "    self.next = None\n",
        "    self.prev = None\n",
        "  def __call__(self, input_size=None, batch_size=None):\n",
        "    self.input_size = input_size if input_size is not None else self.prev.size\n",
        "    self.batch_size = batch_size if batch_size is not None else self.prev.batch_size\n",
        "    self.weights = np.random.randn(self.input_size,self.size)\n",
        "    self.bias = np.zeros((self.batch_size,self.size))\n",
        "\n",
        "  def forward(self, a_layer_prev):\n",
        "    z_layer = np.dot(a_layer_prev, self.weights) + self.bias\n",
        "    a_layer = self.activation_fn()(z_layer)\n",
        "    return z_layer, a_layer\n",
        "\n",
        "  def backward(self, delta, z_layer,a_layer_prev, learning_rate, layer_no):\n",
        "    delta = delta if layer_no==0 else np.dot(delta, self.next.weights.T)\n",
        "    error_prime = delta * self.activation_fn().prime(z_layer)\n",
        "    weights_error = np.dot(a_layer_prev.T, error_prime)\n",
        "    bias_error = error_prime\n",
        "    self.weights -= weights_error*learning_rate\n",
        "    self.bias -= bias_error*learning_rate\n",
        "    return error_prime\n",
        "\n",
        "  def add(self, new_layer):\n",
        "    self.next = new_layer\n",
        "    new_layer.prev = self\n",
        "    return new_layer\n",
        "\n",
        "\n",
        "class model:\n",
        "  def __init__(self, optimizer=None, learning_rate=0.1, loss=mse, batch_size=1):\n",
        "    self.optimzer = optimizer#not being implemented now\n",
        "    self.learning_rate = learning_rate\n",
        "    self.loss = loss\n",
        "    self.batch_size = batch_size\n",
        "    self.layer = None\n",
        "    self.start_layer = None\n",
        "    self.a_graph = []\n",
        "    self.z_graph = []\n",
        "\n",
        "  def add(self, new_layer, feature_size=None):\n",
        "    if feature_size != None:\n",
        "      new_layer(input_size=feature_size, batch_size=self.batch_size)# call method doesn't return anything\n",
        "      self.start_layer = new_layer\n",
        "      self.layer = self.start_layer\n",
        "    else:\n",
        "      self.layer = self.layer.add(new_layer)\n",
        "      new_layer()\n",
        "    #return self\n",
        "\n",
        "\n",
        "  def forward_pass(self, X, Y):\n",
        "    layer=self.start_layer\n",
        "    a_layer = X\n",
        "    while layer is not None:\n",
        "      z_layer, a_layer = layer.forward(a_layer)\n",
        "      self.z_graph.append(z_layer)\n",
        "      self.a_graph.append(a_layer)\n",
        "      layer=layer.next\n",
        "    return a_layer\n",
        "\n",
        "  def backprop(self, X, Y, delta):\n",
        "    layer_backprop=self.layer#starting from the last\n",
        "    a_layer = X\n",
        "    layer_no=0\n",
        "    while layer_backprop.prev is not None:\n",
        "      delta = layer_backprop.backward(delta, self.z_graph[-1-layer_no], self.a_graph[-2-layer_no], self.learning_rate, layer_no)# error_prime for each layer which feeds to next layer\n",
        "      layer_backprop=layer_backprop.prev\n",
        "      layer_no+=1\n",
        "    layer_backprop.backward(delta, self.z_graph[-1-layer_no], X, self.learning_rate, layer_no)\n",
        "\n",
        "  def train(self, X, Y, epochs, X_test=None, Y_test=None):\n",
        "    batch_count=int(np.shape(X)[0]/self.batch_size)\n",
        "    for epoch in range(epochs):\n",
        "      for batch in range(batch_count):\n",
        "        x_batch=X[batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        y_batch=Y[batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "        y_generated = self.forward_pass(x_batch,y_batch)\n",
        "        error = self.loss()(y_generated, y_batch)\n",
        "        delta=self.loss().prime(y_generated, y_batch)# error_prime for last layer\n",
        "        self.backprop(x_batch, y_batch, delta)\n",
        "      if (epoch+1)%int(epochs*0.1)==0 : print(\"\\nepoch: \", epoch, \"\\terror: \", sum(error)[0], \"\\ty_generated: \", y_generated, \"\\ty_batch: \", y_batch)\n",
        "      if X_test is not None:\n",
        "        batch_count_test=int(np.shape(X_test)[0]/self.batch_size)\n",
        "        for batch in range(batch_count_test):\n",
        "          y_generated_test = self.predict(X_test)\n",
        "          error_test = self.loss(y_generated_test, Y_test)\n",
        "\n",
        "  def predict(self, X):\n",
        "    batch_count=int(np.shape(X)[0]/self.batch_size)\n",
        "    for batch in range(batch_count):\n",
        "      x_batch=X[batch*self.batch_size:(batch+1)*self.batch_size]\n",
        "      a_layer = X\n",
        "      layer=self.start_layer\n",
        "      while layer is not None:\n",
        "        _, a_layer = layer.forward(a_layer,self.batch_size)\n",
        "        layer=layer.next\n",
        "      return a_layer\n",
        "\n",
        "\n",
        "Model=model(optimizer=None, learning_rate=0.1, loss=mse, batch_size=1)\n",
        "Model.add(layer(size=10, activation_fn=sigmoid), feature_size=np.shape(x_input)[-1])# only for first layer\n",
        "Model.add(layer(size=10, activation_fn=sigmoid))\n",
        "Model.add(layer(size=np.shape(y_output)[-1], activation_fn=linear))\n",
        "Model.train(x_input,y_output, epochs=1000)"
      ],
      "metadata": {
        "id": "zxF4UL1kaJ8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295f36c8-8c82-4ade-9804-9beccca3e82e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "epoch:  99 \terror:  0.0029645103015735912 \ty_generated:  [[0.84344849]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  199 \terror:  0.0001743410276590889 \ty_generated:  [[0.9017756]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  299 \terror:  1.719156359787191e-05 \ty_generated:  [[0.91458491]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  399 \terror:  1.2521386606602395e-05 \ty_generated:  [[0.91544435]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  499 \terror:  1.7557246070689827e-05 \ty_generated:  [[0.91452288]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  599 \terror:  2.3443897720396954e-05 \ty_generated:  [[0.91360116]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  699 \terror:  2.8522104358206737e-05 \ty_generated:  [[0.91289586]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  799 \terror:  3.248573599019475e-05 \ty_generated:  [[0.91238814]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  899 \terror:  3.539014365636728e-05 \ty_generated:  [[0.91203552]] \ty_batch:  [[0.92044863]]\n",
            "\n",
            "epoch:  999 \terror:  3.738784876555513e-05 \ty_generated:  [[0.91180133]] \ty_batch:  [[0.92044863]]\n"
          ]
        }
      ]
    }
  ]
}